{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1094,"sourceType":"datasetVersion","datasetId":571}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T04:40:01.496872Z","iopub.execute_input":"2025-06-26T04:40:01.497159Z","iopub.status.idle":"2025-06-26T04:40:03.618013Z","shell.execute_reply.started":"2025-06-26T04:40:01.497139Z","shell.execute_reply":"2025-06-26T04:40:03.617190Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/anime-recommendations-database/rating.csv\n/kaggle/input/anime-recommendations-database/anime.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ndcg_score, average_precision_score\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix # For ALS, representing user-item matrix efficiently\n\n# --- 1. Load and Preprocess Data ---\n# Load datasets (assuming the paths are correct in your environment)\nanime_df = pd.read_csv('/kaggle/input/anime-recommendations-database/anime.csv', delimiter=',')\nrating_df = pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv', delimiter=',')\n\n# Clean rating data by removing -1 and averaging duplicate ratings\nrating_clean = rating_df[rating_df['rating'] != -1]\nrating_clean = rating_clean.groupby(['user_id', 'anime_id']).agg({'rating': 'mean'}).reset_index()\n\n# Merge anime and rating data for content features\nmerged_content_df = pd.merge(rating_clean, anime_df, on='anime_id', how='inner')\n\n# Prepare the data for embedding for TF-IDF: Combine name, genre, type, and episodes\n# Handle potential NaN values by filling them with empty strings\nanime_df['name'] = anime_df['name'].fillna('')\nanime_df['genre'] = anime_df['genre'].fillna('')\nanime_df['type'] = anime_df['type'].fillna('')\nanime_df['episodes'] = anime_df['episodes'].fillna('')\n\n# Remove commas from the 'genre' column for TF-IDF\nanime_df['genre'] = anime_df['genre'].str.replace(',', ' ')\n\n\n# Create a combined text string for each anime for TF-IDF\nanime_combined_features = []\nfor index, row in anime_df.iterrows():\n    combined_text = f\"{row['name']} genre: {row['genre']} type: {row['type']} episodes: {row['episodes']}\"\n    anime_combined_features.append(combined_text)\n\nanime_id_to_name = dict(zip(anime_df['anime_id'], anime_df['name'])) # Still useful for displaying recommendations\nname_to_anime_id = dict(zip(anime_df['name'], anime_df['anime_id'])) # Still useful for reverse lookup\n\n\n# Create mappings for unique user IDs and anime IDs for the user-item matrix (for ALS)\n# This is crucial for creating a sparse matrix for collaborative filtering\nunique_user_ids = rating_clean['user_id'].unique()\nunique_anime_ids = anime_df['anime_id'].unique()\n\nuser_to_idx = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\nanime_to_idx = {anime_id: idx for idx, anime_id in enumerate(unique_anime_ids)}\nidx_to_anime = {idx: anime_id for anime_id, idx in anime_to_idx.items()}\n\n\n# --- 2. TF-IDF Implementation (Content-Based Recommendation) ---\nprint(\"Building TF-IDF model...\")\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features for performance\ntfidf_matrix = tfidf_vectorizer.fit_transform(anime_combined_features)\nprint(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n\n# Calculate cosine similarity based on TF-IDF vectors\nprint(\"Calculating cosine similarity matrix based on TF-IDF...\")\ntfidf_similarity_matrix = cosine_similarity(tfidf_matrix)\nprint(\"TF-IDF cosine similarity matrix calculated.\")\n\n# Create mappings from anime_id to its index in the TF-IDF matrix\ntfidf_anime_id_to_index = {anime_id: idx for idx, anime_id in enumerate(anime_df['anime_id'])}\ntfidf_index_to_anime_id = {idx: anime_id for anime_id, idx in tfidf_anime_id_to_index.items()}\n\n\ndef get_tfidf_nearest_neighbors(anime_id: int, similarity_matrix: np.ndarray, top_n: int = 10):\n    \"\"\"\n    Finds the top_n most similar animes for a given anime_id based on TF-IDF similarity.\n    Excludes the anime itself.\n    \"\"\"\n    if anime_id not in tfidf_anime_id_to_index:\n        return []\n\n    anime_idx = tfidf_anime_id_to_index[anime_id]\n    similarities = similarity_matrix[anime_idx]\n\n    # Get indices of top_n+1 most similar items (including itself)\n    # Use argsort to get indices, then reverse to get descending order of similarity\n    top_similar_indices = similarities.argsort()[::-1][1:top_n + 1] # [1:] to exclude itself\n\n    # Map indices back to anime_ids and their similarity scores\n    nearest_neighbors = []\n    for idx in top_similar_indices:\n        neighbor_anime_id = tfidf_index_to_anime_id[idx]\n        neighbor_similarity = similarities[idx]\n        nearest_neighbors.append((neighbor_anime_id, neighbor_similarity))\n\n    return nearest_neighbors\n\n\ndef recommend_by_tfidf(user_id: int, rating_data: pd.DataFrame, similarity_matrix: np.ndarray, top_k: int = 10):\n    \"\"\"\n    Generates content-based recommendations for a user based on their watched anime and similar items\n    using TF-IDF cosine similarity.\n    \"\"\"\n    user_watched_anime = rating_data[rating_data['user_id'] == user_id]\n    if user_watched_anime.empty:\n        return []\n\n    recommended_anime_scores = {}\n    already_watched_anime_ids = set(user_watched_anime['anime_id'].tolist())\n\n    for _, row in user_watched_anime.iterrows():\n        watched_anime_id = row['anime_id']\n        watched_rating = row['rating']\n\n        if watched_anime_id not in tfidf_anime_id_to_index:\n            continue\n\n        neighbors = get_tfidf_nearest_neighbors(watched_anime_id, similarity_matrix, top_n=50)\n\n        for neighbor_anime_id, similarity_score in neighbors:\n            if neighbor_anime_id not in already_watched_anime_ids and neighbor_anime_id in anime_id_to_name:\n                if neighbor_anime_id not in recommended_anime_scores:\n                    recommended_anime_scores[neighbor_anime_id] = 0\n                recommended_anime_scores[neighbor_anime_id] += similarity_score * watched_rating # Weighted by user's rating\n\n    sorted_recommendations = sorted(recommended_anime_scores.items(), key=lambda item: item[1], reverse=True)\n\n    final_recommendations = []\n    for anime_id, score in sorted_recommendations[:top_k]:\n        anime_name = anime_id_to_name.get(anime_id, f\"Unknown Anime (ID: {anime_id})\")\n        final_recommendations.append({'anime_id': anime_id, 'name': anime_name, 'score': score})\n\n    return final_recommendations\n\n\n# --- 3. ALS (Alternating Least Squares) Conceptual Approach (Collaborative Filtering) ---\n\nprint(\"\\n--- ALS (Alternating Least Squares) Conceptual Approach ---\")\nprint(\"Preparing user-item matrix for collaborative filtering...\")\n\n# Create the user-item interaction matrix (sparse matrix is essential for large datasets)\n# Filter rating_clean to only include users and animes present in our mappings\nfiltered_ratings = rating_clean[\n    rating_clean['user_id'].isin(user_to_idx) &\n    rating_clean['anime_id'].isin(anime_to_idx)\n].copy()\n\n# Map original user_id and anime_id to their respective indices\nfiltered_ratings['user_idx'] = filtered_ratings['user_id'].map(user_to_idx)\nfiltered_ratings['anime_idx'] = filtered_ratings['anime_id'].map(anime_to_idx)\n\n# Create a sparse matrix: rows are users, columns are animes, values are ratings\n# This matrix will be used by ALS models\nuser_item_matrix = csr_matrix((\n    filtered_ratings['rating'],\n    (filtered_ratings['user_idx'], filtered_ratings['anime_idx'])\n), shape=(len(unique_user_ids), len(unique_anime_ids)))\n\nprint(f\"User-item matrix shape: {user_item_matrix.shape} (users x animes)\")\nprint(\"This sparse matrix is the primary input for ALS models.\")\n\nprint(\"\\n--- How ALS works conceptually: ---\")\nprint(\"ALS (Alternating Least Squares) is a matrix factorization algorithm primarily used for collaborative filtering.\")\nprint(\"It decomposes the user-item interaction matrix (like the one created above) into two lower-dimensional matrices:\")\nprint(\"1. User latent features matrix (U)\")\nprint(\"2. Item latent features matrix (V)\")\nprint(\"Such that their product approximates the original user-item matrix (R ≈ U * V^T).\")\nprint(\"These latent features capture underlying preferences or characteristics that are not explicitly observed.\")\nprint(\"The algorithm iteratively optimizes U and V by holding one fixed and solving for the other, alternating until convergence.\")\n\nprint(\"\\n--- Generating recommendations with ALS (Conceptual): ---\")\nprint(\"In a practical implementation, you would use a library like 'implicit' (for implicit feedback, but can adapt for explicit) or 'LightFM':\")\nprint(\"1. Initialize and train an ALS model on the 'user_item_matrix'.\")\nprint(\"   e.g., `model = implicit.als.AlternatingLeastSquares(...)`\")\nprint(\"   e.g., `model.fit(user_item_matrix)`\")\nprint(\"2. Once trained, you can use the model to recommend items for a specific user:\")\nprint(\"   e.g., `recommendations = model.recommend(user_id_idx, user_item_matrix[user_id_idx], N=10)`\")\nprint(\"   The `recommend` method usually considers items the user has not yet interacted with.\")\nprint(\"3. These recommendations would then be evaluated using MAP@K and NDCG@K, similar to the TF-IDF approach.\")\nprint(\"\\n*Note: A full, runnable ALS implementation is beyond the scope of a single, self-contained Python file without external library installations.*\")\n\n\n# --- 4. Evaluation (MAP@10 and NDCG@10) ---\n\n# To evaluate, we need to split data into train and test sets.\n# We'll use the train set to generate recommendations and the test set as ground truth.\ntrain_rating, test_rating = train_test_split(rating_clean, test_size=0.2, random_state=42)\n\n# Ensure all anime_ids in test_rating are present in our processed anime_df\ntest_rating = test_rating[test_rating['anime_id'].isin(anime_df['anime_id'])]\n\ndef calculate_map_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Mean Average Precision at K (MAP@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    relevant_count = 0\n    precision_sum = 0.0\n\n    for i, item_id in enumerate(recommended_items[:k]):\n        if item_id in ground_truth_items:\n            relevant_count += 1\n            precision_sum += relevant_count / (i + 1)\n    \n    return precision_sum / min(len(ground_truth_items), k) if relevant_count > 0 else 0.0\n\n\ndef calculate_ndcg_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Normalized Discounted Cumulative Gain at K (NDCG@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    relevance = [1 if item_id in ground_truth_items else 0 for item_id in recommended_items[:k]]\n\n    # Ensure y_true and y_score have the same length (k)\n    # Pad relevance if less than k\n    if len(relevance) < k:\n        relevance.extend([0] * (k - len(relevance)))\n    \n    y_true = np.asarray([relevance])\n    y_score = np.asarray([np.arange(k, 0, -1)]) # A simple descending score for ranking\n\n    try:\n        return ndcg_score(y_true, y_score)\n    except ValueError as e:\n        print(f\"NDCG calculation error for a user: {e}. Returning 0.0.\")\n        return 0.0\n\n\nprint(\"\\nStarting evaluation for TF-IDF based recommendations (MAP@10 and NDCG@10)...\")\nuser_ids_to_evaluate = test_rating['user_id'].unique()\nmap_scores_tfidf = []\nndcg_scores_tfidf = []\n\n# Filter users who have ratings in the training set\nusers_with_train_data = train_rating['user_id'].unique()\nuser_ids_for_evaluation = [uid for uid in user_ids_to_evaluate if uid in users_with_train_data]\n\n# --- Optimization: Limit the number of users for evaluation ---\nMAX_EVAL_USERS = 500 # Adjust this number based on desired evaluation speed and thoroughness\nif len(user_ids_for_evaluation) > MAX_EVAL_USERS:\n    print(f\"Limiting evaluation to the first {MAX_EVAL_USERS} users for faster results.\")\n    user_ids_for_evaluation = user_ids_for_evaluation[:MAX_EVAL_USERS]\n# --- End Optimization ---\n\n\nif not user_ids_for_evaluation:\n    print(\"No users with sufficient data in both train and test sets for evaluation. Please check your data split.\")\nelse:\n    for user_id in tqdm(user_ids_for_evaluation, desc=\"Evaluating users (TF-IDF)\"):\n        # Get ground truth from the test set\n        # Consider ratings >= 7 as \"relevant\"\n        ground_truth_anime_ids = set(test_rating[\n            (test_rating['user_id'] == user_id) & (test_rating['rating'] >= 7)\n        ]['anime_id'].tolist())\n\n        if not ground_truth_anime_ids:\n            continue # Skip users with no relevant items in test set in the test set\n\n        # Generate recommendations based on the training data using TF-IDF\n        recommendations_tfidf = recommend_by_tfidf(user_id, train_rating, tfidf_similarity_matrix, top_k=10)\n        recommended_anime_ids_tfidf = [rec['anime_id'] for rec in recommendations_tfidf]\n\n        # Calculate MAP@10\n        map_scores_tfidf.append(calculate_map_at_k(recommended_anime_ids_tfidf, ground_truth_anime_ids, k=10))\n\n        # Calculate NDCG@10\n        ndcg_scores_tfidf.append(calculate_ndcg_at_k(recommended_anime_ids_tfidf, ground_truth_anime_ids, k=10))\n\n    avg_map_at_10_tfidf = np.mean(map_scores_tfidf) if map_scores_tfidf else 0\n    avg_ndcg_at_10_tfidf = np.mean(ndcg_scores_tfidf) if ndcg_scores_tfidf else 0\n\n    print(f\"\\n--- TF-IDF Evaluation Results ---\")\n    print(f\"Average MAP@10 (TF-IDF): {avg_map_at_10_tfidf:.4f}\")\n    print(f\"Average NDCG@10 (TF-IDF): {avg_ndcg_at_10_tfidf:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T04:40:03.619421Z","iopub.execute_input":"2025-06-26T04:40:03.619749Z","iopub.status.idle":"2025-06-26T04:42:22.562855Z","shell.execute_reply.started":"2025-06-26T04:40:03.619730Z","shell.execute_reply":"2025-06-26T04:42:22.562017Z"}},"outputs":[{"name":"stdout","text":"Building TF-IDF model...\nTF-IDF matrix shape: (12294, 5000)\nCalculating cosine similarity matrix based on TF-IDF...\nTF-IDF cosine similarity matrix calculated.\n\n--- ALS (Alternating Least Squares) Conceptual Approach ---\nPreparing user-item matrix for collaborative filtering...\nUser-item matrix shape: (69600, 12294) (users x animes)\nThis sparse matrix is the primary input for ALS models.\n\n--- How ALS works conceptually: ---\nALS (Alternating Least Squares) is a matrix factorization algorithm primarily used for collaborative filtering.\nIt decomposes the user-item interaction matrix (like the one created above) into two lower-dimensional matrices:\n1. User latent features matrix (U)\n2. Item latent features matrix (V)\nSuch that their product approximates the original user-item matrix (R ≈ U * V^T).\nThese latent features capture underlying preferences or characteristics that are not explicitly observed.\nThe algorithm iteratively optimizes U and V by holding one fixed and solving for the other, alternating until convergence.\n\n--- Generating recommendations with ALS (Conceptual): ---\nIn a practical implementation, you would use a library like 'implicit' (for implicit feedback, but can adapt for explicit) or 'LightFM':\n1. Initialize and train an ALS model on the 'user_item_matrix'.\n   e.g., `model = implicit.als.AlternatingLeastSquares(...)`\n   e.g., `model.fit(user_item_matrix)`\n2. Once trained, you can use the model to recommend items for a specific user:\n   e.g., `recommendations = model.recommend(user_id_idx, user_item_matrix[user_id_idx], N=10)`\n   The `recommend` method usually considers items the user has not yet interacted with.\n3. These recommendations would then be evaluated using MAP@K and NDCG@K, similar to the TF-IDF approach.\n\n*Note: A full, runnable ALS implementation is beyond the scope of a single, self-contained Python file without external library installations.*\n\nStarting evaluation for TF-IDF based recommendations (MAP@10 and NDCG@10)...\nLimiting evaluation to the first 500 users for faster results.\n","output_type":"stream"},{"name":"stderr","text":"Evaluating users (TF-IDF): 100%|██████████| 500/500 [02:04<00:00,  4.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- TF-IDF Evaluation Results ---\nAverage MAP@10 (TF-IDF): 0.0155\nAverage NDCG@10 (TF-IDF): 0.1216\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}