{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1094,"sourceType":"datasetVersion","datasetId":571},{"sourceId":12285968,"sourceType":"datasetVersion","datasetId":7742884}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bd5af2ec","cell_type":"markdown","source":"### Load libraries\n```{r}","metadata":{}},{"id":"2b0fd76f","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ndcg_score, average_precision_score\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# --- 1. Load and Preprocess Data ---\n# Load datasets (assuming the paths are correct in your environment)\nanime_df = pd.read_csv('/kaggle/input/anime-recommendations-database/anime.csv', delimiter=',')\nrating_df = pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv', delimiter=',')\n\n# Clean rating data by removing -1 and averaging duplicate ratings\nrating_clean = rating_df[rating_df['rating'] != -1]\nrating_clean = rating_clean.groupby(['user_id', 'anime_id']).agg({'rating': 'mean'}).reset_index()\n\n# Merge anime and rating data\nmerged_df = pd.merge(rating_clean, anime_df, on='anime_id', how='inner')\n\n# Prepare the data for embedding: Combine name, genre, type, and episodes\n# Handle potential NaN values by filling them with empty strings\nanime_df['name'] = anime_df['name'].fillna('')\nanime_df['genre'] = anime_df['genre'].fillna('')\nanime_df['type'] = anime_df['type'].fillna('')\nanime_df['episodes'] = anime_df['episodes'].fillna('')\n\n# Remove commas from the 'genre' column\nanime_df['genre'] = anime_df['genre'].str.replace(',', ' ')\n\n\n# Create a combined text string for each anime\nanime_combined_features = []\nfor index, row in anime_df.iterrows():\n    combined_text = f\"{row['name']} genre: {row['genre']} type: {row['type']} episodes: {row['episodes']}\"\n    anime_combined_features.append(combined_text)\n\nanime_id_to_name = dict(zip(anime_df['anime_id'], anime_df['name'])) # Still useful for displaying recommendations\nname_to_anime_id = dict(zip(anime_df['name'], anime_df['anime_id'])) # Still useful for reverse lookup\n\n\n# --- 2. Load Pretrained Model and Generate Embeddings ---\ndef load_pretrained_model(model_name: str):\n    \"\"\"Loads a SentenceTransformer model.\"\"\"\n    return SentenceTransformer(model_name)\n\nprint(\"Loading SentenceTransformer model...\")\nmodel = load_pretrained_model('all-MiniLM-L6-v2') # Smaller, faster model for demonstration\n\nprint(\"Generating anime embeddings (this might take a while for large datasets)...\")\n# Use the combined features for embedding generation\nanime_embeddings = model.encode(anime_combined_features, show_progress_bar=True)\nprint(f\"Generated {anime_embeddings.shape[0]} embeddings of dimension {anime_embeddings.shape[1]}\")\n\n# Create a mapping from anime_id to its embedding\nanime_id_to_embedding = {\n    anime_df.loc[i, 'anime_id']: anime_embeddings[i]\n    for i in range(len(anime_df))\n}\n\n\n# --- 3. Calculate Cosine Similarity ---\n# For efficient calculation, we will calculate similarity between all anime embeddings\nprint(\"Calculating cosine similarity matrix...\")\nanime_similarity_matrix = cosine_similarity(anime_embeddings)\nprint(\"Cosine similarity matrix calculated.\")\n\n# Create a mapping from index in similarity matrix back to anime_id\nindex_to_anime_id = {i: anime_df.loc[i, 'anime_id'] for i in range(len(anime_df))}\nanime_id_to_index = {anime_df.loc[i, 'anime_id']: i for i in range(len(anime_df))}\n\n\n# --- 4. Implement Nearest Neighbors ---\ndef get_nearest_neighbors(anime_id: int, similarity_matrix: np.ndarray, top_n: int = 10):\n    \"\"\"\n    Finds the top_n most similar animes for a given anime_id.\n    Excludes the anime itself.\n    \"\"\"\n    if anime_id not in anime_id_to_index:\n        return []\n\n    anime_idx = anime_id_to_index[anime_id]\n    similarities = similarity_matrix[anime_idx]\n\n    # Get indices of top_n+1 most similar items (including itself)\n    # Use argsort to get indices, then reverse to get descending order of similarity\n    top_similar_indices = similarities.argsort()[::-1][1:top_n + 1] # [1:] to exclude itself\n\n    # Map indices back to anime_ids and their similarity scores\n    nearest_neighbors = []\n    for idx in top_similar_indices:\n        neighbor_anime_id = index_to_anime_id[idx]\n        neighbor_similarity = similarities[idx]\n        nearest_neighbors.append((neighbor_anime_id, neighbor_similarity))\n\n    return nearest_neighbors\n\n# Example usage:\n# print(\"\\nNearest neighbors for Anime A (ID 1):\")\n# print(get_nearest_neighbors(1, anime_similarity_matrix))\n\n\n# --- 5. Generate Recommendations ---\ndef recommend_for_user(user_id: int, rating_data: pd.DataFrame, similarity_matrix: np.ndarray, top_k: int = 10):\n    \"\"\"\n    Generates recommendations for a user based on their watched anime and similar items.\n    This is a basic user-based collaborative filtering approach using item-item similarity.\n    \"\"\"\n    user_watched_anime = rating_data[rating_data['user_id'] == user_id]\n    if user_watched_anime.empty:\n        print(f\"User {user_id} has no watched anime data.\")\n        return []\n\n    recommended_anime_scores = {}\n    already_watched_anime_ids = set(user_watched_anime['anime_id'].tolist())\n\n    for _, row in user_watched_anime.iterrows():\n        watched_anime_id = row['anime_id']\n        watched_rating = row['rating']\n\n        # Get nearest neighbors for the watched anime\n        neighbors = get_nearest_neighbors(watched_anime_id, similarity_matrix, top_n=50) # Get more neighbors to choose from\n\n        for neighbor_anime_id, similarity_score in neighbors:\n            # Only recommend if not already watched by the user\n            if neighbor_anime_id not in already_watched_anime_ids:\n                # Simple aggregation: sum of (similarity * user_rating_for_watched_item)\n                # You can use more sophisticated weighted averages here\n                if neighbor_anime_id not in recommended_anime_scores:\n                    recommended_anime_scores[neighbor_anime_id] = 0\n                recommended_anime_scores[neighbor_anime_id] += similarity_score * watched_rating\n\n    # Sort recommendations by score in descending order\n    sorted_recommendations = sorted(recommended_anime_scores.items(), key=lambda item: item[1], reverse=True)\n\n    # Get the top_k recommendations, retrieving their names\n    final_recommendations = []\n    for anime_id, score in sorted_recommendations[:top_k]:\n        anime_name = anime_id_to_name.get(anime_id, f\"Unknown Anime (ID: {anime_id})\")\n        final_recommendations.append({'anime_id': anime_id, 'name': anime_name, 'score': score})\n\n    return final_recommendations\n\n# Example usage:\n# user_id_to_test = 1\n# recommendations = recommend_for_user(user_id_to_test, rating_clean, anime_similarity_matrix, top_k=10)\n# print(f\"\\nRecommendations for User {user_id_to_test}:\")\n# for rec in recommendations:\n#     print(f\"  - {rec['name']} (ID: {rec['anime_id']}) Score: {rec['score']:.2f}\")\n\n\n# --- 6. Evaluation (MAP@10 and NDCG@10) ---\n\n# To evaluate, we need to split data into train and test sets.\n# We'll use the train set to generate recommendations and the test set as ground truth.\ntrain_rating, test_rating = train_test_split(rating_clean, test_size=0.2, random_state=42)\n\n# Ensure all anime_ids in test_rating are present in anime_id_to_index for embedding lookups\n# Filter out test ratings where anime_id is not in our processed anime_df\ntest_rating = test_rating[test_rating['anime_id'].isin(anime_df['anime_id'])]\n\ndef calculate_map_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Mean Average Precision at K (MAP@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    relevant_count = 0\n    precision_sum = 0.0\n\n    for i, item_id in enumerate(recommended_items[:k]):\n        if item_id in ground_truth_items:\n            relevant_count += 1\n            precision_sum += relevant_count / (i + 1)\n    \n    return precision_sum / min(len(ground_truth_items), k) if relevant_count > 0 else 0.0\n\n\ndef calculate_ndcg_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Normalized Discounted Cumulative Gain at K (NDCG@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    # Create a relevance score list for NDCG calculation\n    # 1 if item is relevant, 0 otherwise\n    relevance = [1 if item_id in ground_truth_items else 0 for item_id in recommended_items[:k]]\n\n    # For NDCG, we need a list of scores for actual and ideal ordering.\n    # The `ndcg_score` function from sklearn expects a 2D array for y_true (relevance)\n    # and y_score (predicted scores/rankings).\n    # Since our recommendations are already ranked, we can use a simple array.\n    # The ideal_dcg assumes all ground_truth_items are ranked perfectly at the top.\n    \n    # y_true should represent the relevance of each item in the *recommended list*.\n    # y_score can be a list of dummy scores, as long as it preserves the ranking.\n    # We'll use the relevance list itself for y_true and a descending range for y_score\n    # to indicate the ranking.\n    \n    # Ensure y_true and y_score have the same length (k)\n    y_true = np.asarray([relevance])\n    # A simple descending score for the recommended items\n    y_score = np.asarray([np.arange(k, 0, -1)])\n    \n    # If the length of relevance is less than k, pad with zeros\n    if len(relevance) < k:\n        relevance.extend([0] * (k - len(relevance)))\n    \n    try:\n        # Use relevance scores directly for y_true for `ndcg_score`\n        # and a proxy for the predicted scores (e.g., inverse of rank)\n        return ndcg_score([relevance], [list(range(k, 0, -1))])\n    except ValueError as e:\n        # This error can occur if all y_true values are zero (no relevant items)\n        # or if the dimensions don't match.\n        # If no relevant items are found in recommendations, NDCG is 0.\n        print(f\"NDCG calculation error: {e}. Returning 0.0.\")\n        return 0.0\n\n\nprint(\"\\nStarting evaluation for MAP@10 and NDCG@10...\")\nuser_ids_to_evaluate = test_rating['user_id'].unique()\nmap_scores = []\nndcg_scores = []\n\n# Filter users who have ratings in the training set\nusers_with_train_data = train_rating['user_id'].unique()\nuser_ids_for_evaluation = [uid for uid in user_ids_to_evaluate if uid in users_with_train_data]\n\nif not user_ids_for_evaluation:\n    print(\"No users with sufficient data in both train and test sets for evaluation. Please check your data split.\")\nelse:\n    for user_id in tqdm(user_ids_for_evaluation, desc=\"Evaluating users\"):\n        # Get ground truth from the test set\n        ground_truth_anime_ids = set(test_rating[\n            (test_rating['user_id'] == user_id) & (test_rating['rating'] >= 7) # Consider ratings >= 7 as \"relevant\"\n        ]['anime_id'].tolist())\n\n        if not ground_truth_anime_ids:\n            continue # Skip users with no relevant items in test set\n\n        # Generate recommendations based on the training data\n        # Ensure 'rating_clean' is passed correctly, or 'train_rating' for generating recommendations\n        recommendations = recommend_for_user(user_id, train_rating, anime_similarity_matrix, top_k=10)\n        recommended_anime_ids = [rec['anime_id'] for rec in recommendations]\n\n        # Calculate MAP@10\n        map_scores.append(calculate_map_at_k(recommended_anime_ids, ground_truth_anime_ids, k=10))\n\n        # Calculate NDCG@10\n        ndcg_scores.append(calculate_ndcg_at_k(recommended_anime_ids, ground_truth_anime_ids, k=10))\n\n    avg_map_at_10 = np.mean(map_scores) if map_scores else 0\n    avg_ndcg_at_10 = np.mean(ndcg_scores) if ndcg_scores else 0\n\n    print(f\"\\n--- Evaluation Results ---\")\n    print(f\"Average MAP@10: {avg_map_at_10:.4f}\")\n    print(f\"Average NDCG@10: {avg_ndcg_at_10:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:56:17.195463Z","iopub.execute_input":"2025-06-26T07:56:17.196109Z","iopub.status.idle":"2025-06-26T08:41:52.035363Z","shell.execute_reply.started":"2025-06-26T07:56:17.196088Z","shell.execute_reply":"2025-06-26T08:41:52.034451Z"}},"outputs":[{"name":"stdout","text":"Loading SentenceTransformer model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb14b6f6904d4981a2b298fe23af6591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ba92f3ff974c698ca6d5e942c15d95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17773ed48eae45e0b762dcba602f281c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740b824a18184bc982d7c0b2e522cd93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33791145dc4845699ac1d6242107f9b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4277a1b771de49d9a178946392cd6e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46d7919d11a411eb2f9d124073c31ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1a22d3bfce42b29be15cec3b4fbcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b744a82c20f040ffbbce36d6b4c61ddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc8d13401764443ac67a81d8db637a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6189d2ad32c44ae870eda824b994baa"}},"metadata":{}},{"name":"stdout","text":"Generating anime embeddings (this might take a while for large datasets)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/385 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60806bb812ac40a68f12b28e972c564d"}},"metadata":{}},{"name":"stdout","text":"Generated 12294 embeddings of dimension 384\nCalculating cosine similarity matrix...\nCosine similarity matrix calculated.\n\nStarting evaluation for MAP@10 and NDCG@10...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating users: 100%|██████████| 61658/61658 [45:10<00:00, 22.75it/s]  ","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluation Results ---\nAverage MAP@10: 0.0301\nAverage NDCG@10: 0.1643\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"id":"7c18084b-5d8a-4270-abec-17edd2afbe8f","cell_type":"code","source":"# Load and Preprocess Data\n# Load datasets (assuming the paths are correct in your environment)\nanime_df = pd.read_csv('/kaggle/input/anime-recommendations-database/anime.csv', delimiter=',')\nrating_df = pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv', delimiter=',')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1c6935b8-567d-4ddf-83b6-42a5cf69884c","cell_type":"code","source":"# Clean rating data by removing -1 and averaging duplicate ratings\nrating_clean = rating_df[rating_df['rating'] != -1]\nrating_clean = rating_clean.groupby(['user_id', 'anime_id']).agg({'rating': 'mean'}).reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c98535d0-bbd3-4853-a110-4a37f97f1b6e","cell_type":"code","source":"# Merge anime and rating data\nmerged_df = pd.merge(rating_clean, anime_df, on='anime_id', how='inner')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f2ef61d9-e332-47fd-850e-90a95fb32f7d","cell_type":"code","source":"# Prepare the data for embedding: Combine name, genre, type, and episodes\n# Handle potential NaN values by filling them with empty strings\nanime_df['name'] = anime_df['name'].fillna('')\nanime_df['genre'] = anime_df['genre'].fillna('')\nanime_df['type'] = anime_df['type'].fillna('')\nanime_df['episodes'] = anime_df['episodes'].fillna('')\n\n# Remove commas from the 'genre' column\nanime_df['genre'] = anime_df['genre'].str.replace(',', ' ')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d34ea983-2602-421d-982a-26e521f2f548","cell_type":"code","source":"# Create a combined text string for each anime\nanime_combined_features = []\nfor index, row in anime_df.iterrows():\n    combined_text = f\"{row['name']} genre: {row['genre']} type: {row['type']} episodes: {row['episodes']}\"\n    anime_combined_features.append(combined_text)\n\nanime_id_to_name = dict(zip(anime_df['anime_id'], anime_df['name'])) # useful for displaying recommendations\nname_to_anime_id = dict(zip(anime_df['name'], anime_df['anime_id'])) # useful for reverse lookup","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a3b3aa9-cabd-42b6-8963-0aefac26e21e","cell_type":"code","source":"# Load Pretrained Model and Generate Embeddings\ndef load_pretrained_model(model_name: str):\n    \"\"\"Loads a SentenceTransformer model.\"\"\"\n    return SentenceTransformer(model_name)\n\nprint(\"Loading SentenceTransformer model...\")\nmodel = load_pretrained_model('all-MiniLM-L6-v2') # Smaller, faster model for demonstration\n\nprint(\"Generating anime embeddings (this might take a while for large datasets)...\")\n# Use the combined features for embedding generation\nanime_embeddings = model.encode(anime_combined_features, show_progress_bar=True)\nprint(f\"Generated {anime_embeddings.shape[0]} embeddings of dimension {anime_embeddings.shape[1]}\")\n\n# Create a mapping from anime_id to its embedding\nanime_id_to_embedding = {\n    anime_df.loc[i, 'anime_id']: anime_embeddings[i]\n    for i in range(len(anime_df))\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"017d8844-c850-470a-9ee7-e7bf8eea2f47","cell_type":"code","source":"# Calculate Cosine Similarity\n# For efficient calculation, we will calculate similarity between all anime embeddings\nprint(\"Calculating cosine similarity matrix...\")\nanime_similarity_matrix = cosine_similarity(anime_embeddings)\nprint(\"Cosine similarity matrix calculated.\")\n\n# Create a mapping from index in similarity matrix back to anime_id\nindex_to_anime_id = {i: anime_df.loc[i, 'anime_id'] for i in range(len(anime_df))}\nanime_id_to_index = {anime_df.loc[i, 'anime_id']: i for i in range(len(anime_df))}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"32aedbe9-63c0-4ad1-85db-3828ac09b60b","cell_type":"code","source":"# Implement Nearest Neighbors\ndef get_nearest_neighbors(anime_id: int, similarity_matrix: np.ndarray, top_n: int = 10):\n    \"\"\"\n    Finds the top_n most similar animes for a given anime_id.\n    Excludes the anime itself.\n    \"\"\"\n    if anime_id not in anime_id_to_index:\n        return []\n\n    anime_idx = anime_id_to_index[anime_id]\n    similarities = similarity_matrix[anime_idx]\n\n    # Get indices of top_n+1 most similar items (including itself)\n    # Use argsort to get indices, then reverse to get descending order of similarity\n    top_similar_indices = similarities.argsort()[::-1][1:top_n + 1] # [1:] to exclude itself\n\n    # Map indices back to anime_ids and their similarity scores\n    nearest_neighbors = []\n    for idx in top_similar_indices:\n        neighbor_anime_id = index_to_anime_id[idx]\n        neighbor_similarity = similarities[idx]\n        nearest_neighbors.append((neighbor_anime_id, neighbor_similarity))\n\n    return nearest_neighbors","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"87fe19da-4919-444b-8b07-3d1deaa3880b","cell_type":"code","source":"# Example usage:\n# print(\"\\nNearest neighbors for Anime A (ID 1):\")\n# print(get_nearest_neighbors(1, anime_similarity_matrix))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8cd88795-4fb3-436b-9544-368cd9dc9c66","cell_type":"code","source":"# --- 5. Generate Recommendations ---\ndef recommend_for_user(user_id: int, rating_data: pd.DataFrame, similarity_matrix: np.ndarray, top_k: int = 10):\n    \"\"\"\n    Generates recommendations for a user based on their watched anime and similar items.\n    This is a basic user-based collaborative filtering approach using item-item similarity.\n    \"\"\"\n    user_watched_anime = rating_data[rating_data['user_id'] == user_id]\n    if user_watched_anime.empty:\n        print(f\"User {user_id} has no watched anime data.\")\n        return []\n\n    recommended_anime_scores = {}\n    already_watched_anime_ids = set(user_watched_anime['anime_id'].tolist())\n\n    for _, row in user_watched_anime.iterrows():\n        watched_anime_id = row['anime_id']\n        watched_rating = row['rating']\n\n        # Get nearest neighbors for the watched anime\n        neighbors = get_nearest_neighbors(watched_anime_id, similarity_matrix, top_n=50) # Get more neighbors to choose from\n\n        for neighbor_anime_id, similarity_score in neighbors:\n            # Only recommend if not already watched by the user\n            if neighbor_anime_id not in already_watched_anime_ids:\n                # Simple aggregation: sum of (similarity * user_rating_for_watched_item)\n                # You can use more sophisticated weighted averages here\n                if neighbor_anime_id not in recommended_anime_scores:\n                    recommended_anime_scores[neighbor_anime_id] = 0\n                recommended_anime_scores[neighbor_anime_id] += similarity_score * watched_rating\n\n    # Sort recommendations by score in descending order\n    sorted_recommendations = sorted(recommended_anime_scores.items(), key=lambda item: item[1], reverse=True)\n\n    # Get the top_k recommendations, retrieving their names\n    final_recommendations = []\n    for anime_id, score in sorted_recommendations[:top_k]:\n        anime_name = anime_id_to_name.get(anime_id, f\"Unknown Anime (ID: {anime_id})\")\n        final_recommendations.append({'anime_id': anime_id, 'name': anime_name, 'score': score})\n\n    return final_recommendations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b68521a6-9f23-43a5-8795-77da17155acd","cell_type":"code","source":"# Example usage:\n# user_id_to_test = 1\n# recommendations = recommend_for_user(user_id_to_test, rating_clean, anime_similarity_matrix, top_k=10)\n# print(f\"\\nRecommendations for User {user_id_to_test}:\")\n# for rec in recommendations:\n#     print(f\"  - {rec['name']} (ID: {rec['anime_id']}) Score: {rec['score']:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"34297ede-b66a-4978-b7a7-871d4fc95409","cell_type":"code","source":"# --- 6. Evaluation (MAP@10 and NDCG@10) ---\n\n# To evaluate, we need to split data into train and test sets.\n# We'll use the train set to generate recommendations and the test set as ground truth.\ntrain_rating, test_rating = train_test_split(rating_clean, test_size=0.2, random_state=42)\n\n# Ensure all anime_ids in test_rating are present in anime_id_to_index for embedding lookups\n# Filter out test ratings where anime_id is not in our processed anime_df\ntest_rating = test_rating[test_rating['anime_id'].isin(anime_df['anime_id'])]\n\ndef calculate_map_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Mean Average Precision at K (MAP@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    relevant_count = 0\n    precision_sum = 0.0\n\n    for i, item_id in enumerate(recommended_items[:k]):\n        if item_id in ground_truth_items:\n            relevant_count += 1\n            precision_sum += relevant_count / (i + 1)\n    \n    return precision_sum / min(len(ground_truth_items), k) if relevant_count > 0 else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5235f583-ca09-414e-a231-a98703a03e1b","cell_type":"code","source":"def calculate_ndcg_at_k(recommended_items, ground_truth_items, k=10):\n    \"\"\"\n    Calculates Normalized Discounted Cumulative Gain at K (NDCG@K).\n    recommended_items: List of recommended item IDs (ordered by relevance).\n    ground_truth_items: Set of relevant item IDs.\n    \"\"\"\n    if not ground_truth_items:\n        return 0.0\n\n    # Create a relevance score list for NDCG calculation\n    # 1 if item is relevant, 0 otherwise\n    relevance = [1 if item_id in ground_truth_items else 0 for item_id in recommended_items[:k]]\n\n    # For NDCG, we need a list of scores for actual and ideal ordering.\n    # The `ndcg_score` function from sklearn expects a 2D array for y_true (relevance)\n    # and y_score (predicted scores/rankings).\n    # Since our recommendations are already ranked, we can use a simple array.\n    # The ideal_dcg assumes all ground_truth_items are ranked perfectly at the top.\n    \n    # y_true should represent the relevance of each item in the *recommended list*.\n    # y_score can be a list of dummy scores, as long as it preserves the ranking.\n    # We'll use the relevance list itself for y_true and a descending range for y_score\n    # to indicate the ranking.\n    \n    # Ensure y_true and y_score have the same length (k)\n    y_true = np.asarray([relevance])\n    # A simple descending score for the recommended items\n    y_score = np.asarray([np.arange(k, 0, -1)])\n    \n    # If the length of relevance is less than k, pad with zeros\n    if len(relevance) < k:\n        relevance.extend([0] * (k - len(relevance)))\n    \n    try:\n        # Use relevance scores directly for y_true for `ndcg_score`\n        # and a proxy for the predicted scores (e.g., inverse of rank)\n        return ndcg_score([relevance], [list(range(k, 0, -1))])\n    except ValueError as e:\n        # This error can occur if all y_true values are zero (no relevant items)\n        # or if the dimensions don't match.\n        # If no relevant items are found in recommendations, NDCG is 0.\n        print(f\"NDCG calculation error: {e}. Returning 0.0.\")\n        return 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"557615cc-3bf3-4cfe-bf3a-c7507ff63f01","cell_type":"code","source":"print(\"\\nStarting evaluation for MAP@10 and NDCG@10...\")\nuser_ids_to_evaluate = test_rating['user_id'].unique()\nmap_scores = []\nndcg_scores = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"71b5b93b-6c25-419e-9868-eca6bc9319f5","cell_type":"code","source":"# Filter users who have ratings in the training set\nusers_with_train_data = train_rating['user_id'].unique()\nuser_ids_for_evaluation = [uid for uid in user_ids_to_evaluate if uid in users_with_train_data]\n\nif not user_ids_for_evaluation:\n    print(\"No users with sufficient data in both train and test sets for evaluation. Please check your data split.\")\nelse:\n    for user_id in tqdm(user_ids_for_evaluation, desc=\"Evaluating users\"):\n        # Get ground truth from the test set\n        ground_truth_anime_ids = set(test_rating[\n            (test_rating['user_id'] == user_id) & (test_rating['rating'] >= 7) # Consider ratings >= 7 as \"relevant\"\n        ]['anime_id'].tolist())\n\n        if not ground_truth_anime_ids:\n            continue # Skip users with no relevant items in test set\n\n        # Generate recommendations based on the training data\n        # Ensure 'rating_clean' is passed correctly, or 'train_rating' for generating recommendations\n        recommendations = recommend_for_user(user_id, train_rating, anime_similarity_matrix, top_k=10)\n        recommended_anime_ids = [rec['anime_id'] for rec in recommendations]\n\n        # Calculate MAP@10\n        map_scores.append(calculate_map_at_k(recommended_anime_ids, ground_truth_anime_ids, k=10))\n\n        # Calculate NDCG@10\n        ndcg_scores.append(calculate_ndcg_at_k(recommended_anime_ids, ground_truth_anime_ids, k=10))\n\n    avg_map_at_10 = np.mean(map_scores) if map_scores else 0\n    avg_ndcg_at_10 = np.mean(ndcg_scores) if ndcg_scores else 0\n\n    print(f\"\\n--- Evaluation Results ---\")\n    print(f\"Average MAP@10: {avg_map_at_10:.4f}\")\n    print(f\"Average NDCG@10: {avg_ndcg_at_10:.4f}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}